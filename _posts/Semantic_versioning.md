---
layout: post
title: Towards semantic versioning of open pre-trained language model releases on hugging face
excerpt: "Conference"
modified: 5/17/2025, 20:26:24
tags: [Hugging Face, Pre-trained Language Models, Model Versioning Practices, Model Naming Practice, Model Registry]
comments: true
category: blog
---

-------------------------------------------------------------------------------------------------------------------------------------------------------------------------
New Line
---------------------------------------------------------------------------------------------------------------------------------------------------

#Summary of the Findings
The naming conventions of Pre-trained Language Models (PTLMs) on Hugging Face (HF) exhibit significant diversity and complexity. Our analysis reveals that the naming convention of PTLMs on HF encompasses 12 segment types, with identifiers (70.8% of model names), base model (39.8%), and size (34.3%) being the most prevalent. Interestingly, PTLMs mentioning only model size in their name have an average download rate more than 11 times higher than those mentioning only the base model. Through manual analysis, we identified 148 distinct naming conventions across HF repositories, highlighting the wide range of model naming strategies adopted by practitioners. Notably, the naming patterns {identifier}{base-model}{size}, {identifier}{size}, and {identifier}{training-mechanism} are associated with the highest download rates among prevalent conventions. However, our findings indicate no significant relationship between the length of model names and download rates.

Despite the diversity in naming conventions, only a small fraction of PTLMs incorporate version information in their names. Of the 52,227 PTLMs analyzed, merely 3,471 (6.64%) include a version segment. Among these, major versioning, using identifiers like v1, v2, etc., is the predominant strategy, reflecting a software development standard. Specifically, 67% of these PTLMs adopt the major versioning approach, indicating that when version information is present, it primarily reflects major versions only. This trend suggests that significant changes are often not communicated through version identifiers, as evidenced by the 1,282,874 changes observed across 52,227 PTLM repositories, with only 3,471 explicitly indicating changes through versioned model names.

Changes within PTLM repositories are frequent, particularly in model weight files, where 524,419 changes were recorded across the analyzed models. Security-focused tensor files are the most commonly utilized ML framework for storing model weights on HF, averaging 7.88 changes per model. Additionally, 98% of the studied PTLMs include configuration files that detail the base models adapted for PTLMs. Since 2022, the 52,227 PTLM variant releases on HF have all originated from just 299 base models. Among these, the top 15 base models account for 85.80% of releases, with Llama, Bert, and Mistral being the most frequently adapted. Notably, although Llama has seen the most rapid growth, the prevalence of Gemma and Mistral indicates that they are evolving more rapidly when considering their relative age.

Model names and tags on HF exhibit varied indications of variant types, though a substantial number of releases lack explicit variant type information. Specifically, only 12.76% of models indicate variant types in their names, and just 5.63% do so in their tags, leaving 70.72% of PTLM releases without clear variant type specification. Our analysis identified 14 distinct variant type indicators, categorized into Fine-tuning, Deduplication, Quantization, and Knowledge Distillation. Quantized PTLM releases constitute approximately 69.3%, while Fine-tuned PTLM releases account for around 29.6% of the 15,287 PTLM releases on HF.

Training dataset information remains inconsistently documented. Our automatic method for identifying PTLMs with training dataset metadata revealed that 33,964 (65%) out of 52,227 PTLMs have dataset metadata. However, only 24% (8,157) explicitly state their training datasets, while a further manual analysis of model cards indicated that just 12% of the remaining 25,807 PTLMs mention the dataset names, and only 2% provide dataset links. Notably, models resulting from deduplication (24%), fine-tuning (22.3%), and quantization (16.4%) rarely include training datasets, while knowledge distillation has a slightly higher inclusion rate (32.9%).

Moreover, model card documentation is insufficient, with 33% of the 52,227 PTLMs lacking model cards, impeding users' ability to understand and responsibly utilize the models. Among variant types, deduped models most frequently include model cards (82.7%), followed by Quantized (74.5%) and Distilled models (74.1%), while Fine-tuned models have the lowest representation (68.6%). Our findings also indicate that model cards are subject to more frequent changes between major versions (71%) compared to other components, such as base models (13%). Even when comparing minor version updates, model cards remain the most frequently altered attribute (80%).

When examining version changes, our investigation revealed that major version updates generally encompass a broader range of modifications, with an average of 28 unique changes per release, compared to 8 changes for minor updates. Although there is no statistically significant difference in the prevalence of changes between major and minor versions, specific differences are notable in configuration, licensing, and other associated attributes. Analyzing change patterns further, we observed one very strong association, four strong associations, and three moderate associations between pairs of change categories in PTLMs. This nuanced understanding of versioning and change patterns highlights the challenges practitioners face in maintaining consistency and clarity in version documentation and model management on HF.
